{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD_ASSIGNMENT\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create RDD in Three different ways "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World', 'Happy with ABD class and The Big Data Course', 'Hello']\n",
      "3\n",
      "Happy with ABD class and The Big Data Course\n"
     ]
    }
   ],
   "source": [
    "#creating Rdd's using parallelize method \n",
    "rdd_par = spark.sparkContext.parallelize([\"Hello World\", \"Happy with ABD class and The Big Data Course\", \"Hello\"],3)\n",
    "type(rdd_par)\n",
    "print(rdd_par.collect())\n",
    "print(rdd_par.count())\n",
    "print(rdd_par.collect()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World', 'Happy with ABD class and The Big Data Course', 'Hello']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating RDD using transformations\n",
    "rdd_trans = rdd_par.filter(lambda word:word.startswith('H'))\n",
    "rdd_trans.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When to Scale?',\n",
       " '',\n",
       " 'Scaling can be difficult, but absolutely necessary in the growth of a successful data-driven company. There are a few signs that it’s time to implement a scaling platform. When users begin complaining about slow performance, or service outages, it’s time to scale. Don’t wait for the problem to turn into major source of contention in the minds of your customers. This can have a massively negative impact on retaining those customers. If possible, try to anticipate the problem before it becomes severe. In addition to this, increased application latency, slow read queries rises and database writes are also important indicators that a scale is needed.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating RDD using Data Sources\n",
    "rdd_ds = spark.sparkContext.textFile('input')\n",
    "rdd_ds.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read a text file and count number of words in the file using RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the file: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ds = spark.sparkContext.textFile('input.txt')\n",
    "word_rdd = rdd_ds.flatMap(lambda word: word.split(' '))\n",
    "print(\"Number of words in the file: \")\n",
    "word_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a program to find the word frequency in a given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When', 2),\n",
       " ('', 1),\n",
       " ('Scaling', 1),\n",
       " ('difficult,', 1),\n",
       " ('but', 1),\n",
       " ('in', 2),\n",
       " ('growth', 1),\n",
       " ('of', 3),\n",
       " ('data-driven', 1),\n",
       " ('company.', 1),\n",
       " ('are', 2),\n",
       " ('signs', 1),\n",
       " ('it’s', 2),\n",
       " ('platform.', 1),\n",
       " ('performance,', 1),\n",
       " ('service', 1),\n",
       " ('outages,', 1),\n",
       " ('scale.', 1),\n",
       " ('turn', 1),\n",
       " ('into', 1),\n",
       " ('major', 1),\n",
       " ('source', 1),\n",
       " ('minds', 1),\n",
       " ('have', 1),\n",
       " ('negative', 1),\n",
       " ('retaining', 1),\n",
       " ('possible,', 1),\n",
       " ('try', 1),\n",
       " ('before', 1),\n",
       " ('severe.', 1),\n",
       " ('this,', 1),\n",
       " ('increased', 1),\n",
       " ('latency,', 1),\n",
       " ('read', 1),\n",
       " ('database', 1),\n",
       " ('indicators', 1),\n",
       " ('is', 1),\n",
       " ('needed.', 1),\n",
       " ('to', 6),\n",
       " ('Scale?', 1),\n",
       " ('can', 2),\n",
       " ('be', 1),\n",
       " ('absolutely', 1),\n",
       " ('necessary', 1),\n",
       " ('the', 4),\n",
       " ('a', 5),\n",
       " ('successful', 1),\n",
       " ('There', 1),\n",
       " ('few', 1),\n",
       " ('that', 2),\n",
       " ('time', 2),\n",
       " ('implement', 1),\n",
       " ('scaling', 1),\n",
       " ('users', 1),\n",
       " ('begin', 1),\n",
       " ('complaining', 1),\n",
       " ('about', 1),\n",
       " ('slow', 2),\n",
       " ('or', 1),\n",
       " ('Don’t', 1),\n",
       " ('wait', 1),\n",
       " ('for', 1),\n",
       " ('problem', 2),\n",
       " ('contention', 1),\n",
       " ('your', 1),\n",
       " ('customers.', 2),\n",
       " ('This', 1),\n",
       " ('massively', 1),\n",
       " ('impact', 1),\n",
       " ('on', 1),\n",
       " ('those', 1),\n",
       " ('If', 1),\n",
       " ('anticipate', 1),\n",
       " ('it', 1),\n",
       " ('becomes', 1),\n",
       " ('In', 1),\n",
       " ('addition', 1),\n",
       " ('application', 1),\n",
       " ('queries', 1),\n",
       " ('rises', 1),\n",
       " ('and', 1),\n",
       " ('writes', 1),\n",
       " ('also', 1),\n",
       " ('important', 1),\n",
       " ('scale', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rdd = rdd_ds.flatMap(lambda word: word.split(' '))\n",
    "freq_words = word_rdd.map(lambda word: (word, 1))\n",
    "freq_words.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write a program to convert all words in a file to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WHEN TO SCALE?',\n",
       " '',\n",
       " 'SCALING CAN BE DIFFICULT, BUT ABSOLUTELY NECESSARY IN THE GROWTH OF A SUCCESSFUL DATA-DRIVEN COMPANY. THERE ARE A FEW SIGNS THAT IT’S TIME TO IMPLEMENT A SCALING PLATFORM. WHEN USERS BEGIN COMPLAINING ABOUT SLOW PERFORMANCE, OR SERVICE OUTAGES, IT’S TIME TO SCALE. DON’T WAIT FOR THE PROBLEM TO TURN INTO MAJOR SOURCE OF CONTENTION IN THE MINDS OF YOUR CUSTOMERS. THIS CAN HAVE A MASSIVELY NEGATIVE IMPACT ON RETAINING THOSE CUSTOMERS. IF POSSIBLE, TRY TO ANTICIPATE THE PROBLEM BEFORE IT BECOMES SEVERE. IN ADDITION TO THIS, INCREASED APPLICATION LATENCY, SLOW READ QUERIES RISES AND DATABASE WRITES ARE ALSO IMPORTANT INDICATORS THAT A SCALE IS NEEDED.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rdd_upper = rdd_ds.map(lambda word: word.upper())\n",
    "word_rdd_upper.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write a program to convert all words in a file to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when to scale?',\n",
       " '',\n",
       " 'scaling can be difficult, but absolutely necessary in the growth of a successful data-driven company. there are a few signs that it’s time to implement a scaling platform. when users begin complaining about slow performance, or service outages, it’s time to scale. don’t wait for the problem to turn into major source of contention in the minds of your customers. this can have a massively negative impact on retaining those customers. if possible, try to anticipate the problem before it becomes severe. in addition to this, increased application latency, slow read queries rises and database writes are also important indicators that a scale is needed.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rdd_lower = rdd_ds.map(lambda word: word.lower())\n",
    "word_rdd_lower.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Write a program to capitalize first leter of each words in file (use string capitalize() method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When To Scale?',\n",
       " '',\n",
       " 'Scaling Can Be Difficult, But Absolutely Necessary In The Growth Of A Successful Data-driven Company. There Are A Few Signs That It’s Time To Implement A Scaling Platform. When Users Begin Complaining About Slow Performance, Or Service Outages, It’s Time To Scale. Don’t Wait For The Problem To Turn Into Major Source Of Contention In The Minds Of Your Customers. This Can Have A Massively Negative Impact On Retaining Those Customers. If Possible, Try To Anticipate The Problem Before It Becomes Severe. In Addition To This, Increased Application Latency, Slow Read Queries Rises And Database Writes Are Also Important Indicators That A Scale Is Needed.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rdd_capitalized = rdd_ds.map(lambda word: [w.capitalize() for w in word.split(' ')]  )\n",
    "word_rdd_capitalized = word_rdd_capitalized.map(lambda word: ' '.join(word)  )\n",
    "word_rdd_capitalized.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Find the longest length of word from given set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest word length is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rdd_longest = rdd_ds.flatMap(lambda word: word.split(' '))\n",
    "word_rdd_longest = word_rdd_longest.map(lambda word: len(word))\n",
    "print(\"The longest word length is: \")\n",
    "word_rdd_longest.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Map the Registration numbers to corresponding branch. 6000 series BDA, 9000 series HDA, 1000 series ML, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC. Given registration number, generate a key-value pair of Registration Number and Corresponding Branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key, value are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6133, 'BDA'), (3050, 'ES'), (2500, 'VLSI'), (1500, 'ML'), (9050, 'HDA')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = [6133, 3050, 2500, 1500, 9050]\n",
    "rdd_par = spark.sparkContext.parallelize(series)\n",
    "rdd_sequence = rdd_par.map(lambda series: (series, 'ML') if (series >= 1000 and series < 1999)  \n",
    "                           else ((series, 'VLSI') if (series >= 2000 and series < 2999)\n",
    "                           else ((series, 'ES') if (series >= 3000 and series < 3999) \n",
    "                           else ((series, 'MSc') if (series >= 4000 and series < 4999) \n",
    "                           else ((series, 'CC') if (series >= 5000 and series < 5999) \n",
    "                           else ((series, 'BDA') if (series >= 6000 and series < 6999)\n",
    "                           else ((series, 'HDA') if (series >= 9000 and series < 9999) else (series, 'NA'))))))))\n",
    "\n",
    "print(\"Key, value are: \")\n",
    "rdd_sequence.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Text file contain numbers. Numbers are separated by one white space. There is no order to store the numbers. One line may contain one or more numbers. Find the maximum, minimum, sum and mean of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum:  6745292\n",
      "The minimum:  2\n",
      "The sum:  7646030\n",
      "The mean:  364096.66666666674\n"
     ]
    }
   ],
   "source": [
    "rdd_ds = spark.sparkContext.textFile('numbers.txt')\n",
    "rdd_ds_num = rdd_ds.flatMap(lambda n: n.split(' '))\n",
    "rdd_ds_num = rdd_ds_num.map(lambda n: int(n))\n",
    "print(\"The maximum: \", rdd_ds_num.max())\n",
    "print(\"The minimum: \", rdd_ds_num.min())\n",
    "print(\"The sum: \", rdd_ds_num.sum())\n",
    "print(\"The mean: \", rdd_ds_num.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+---------------+----------+\n",
      "|     Name|       dob|     Phone|          email|state code|\n",
      "+---------+----------+----------+---------------+----------+\n",
      "|   VISHNU|20-03-2000|8877986692|test3@gmail.com|        KA|\n",
      "|    ANKIT| 12-6-1620|8877989892|test5@gmail.com|        MP|\n",
      "|PRIYANSHU|30-03-1872|8877986692|test6@gmail.com|        MP|\n",
      "|    GHANA|08-11-1998|8998989892|test1@gmail.com|        AP|\n",
      "|   JYOTHI|20-02-2000|8877989892|test2@gmail.com|        AP|\n",
      "|   MAHESH|13-05-1950|8998989892|test4@gmail.com|        AP|\n",
      "+---------+----------+----------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_ds_citizen = spark.sparkContext.textFile('citizen.txt')\n",
    "rdd_ds_statecode = spark.sparkContext.textFile('statecode.txt')\n",
    "rdd_ds_citizen = rdd_ds_citizen.map(lambda citizen: citizen.split(', '))\n",
    "rdd_ds_statecode = rdd_ds_statecode.map(lambda state: state.split(', '))\n",
    "citizen = spark.createDataFrame(rdd_ds_citizen, ['Name', 'dob', 'Phone', 'email', 'state name'])\n",
    "statecodes = spark.createDataFrame(rdd_ds_statecode, ['state name', 'state code'])\n",
    "citizen.collect()\n",
    "statecodes.collect()\n",
    "citizen.join(statecodes, on='state name', how='left').drop('state name').show()\n",
    "citizen.write.csv('citizen_compressed.csv')\n",
    "citizen.rdd.map(lambda x: x[0] + \",\" + str(x)).repartition(1).saveAsTextFile('Text/citizen.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
